{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "# Đường dẫn đến file PDF của bạn\n",
    "pdf_path = './Data/SKL007296.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    extracted_text = []\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text().strip()\n",
    "        if text:\n",
    "            extracted_text.append((text, page_num + 1))  # page_num + 1 để đánh số trang từ 1\n",
    "    pdf_document.close()\n",
    "    return extracted_text\n",
    "def save_text_with_page_to_file(lines_with_page, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for line, page_num in lines_with_page:\n",
    "            file.write(f\"Số trang {page_num}:\\n{line}\\n\\n\")\n",
    "\n",
    "# Trích xuất nội dung văn bản từ file PDF với số trang\n",
    "text_with_page = extract_pdf_text(pdf_path)\n",
    "\n",
    "# Lưu nội dung vào file văn bản với số trang\n",
    "save_text_with_page_to_file(text_with_page, './output/content.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def combine_lines_and_split_sentences(text_with_page):\n",
    "    combined_lines = []\n",
    "    current_sentence = ''\n",
    "\n",
    "    for text, page_num in text_with_page:\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            # Loại bỏ các dòng chỉ chứa dấu chấm câu và khoảng trắng\n",
    "            if re.match(r'^[\\s]*$', line):\n",
    "                continue\n",
    "            # Xóa khoảng trắng thừa với dấu câu và loại bỏ ký tự đặc biệt và số La Mã\n",
    "            line = re.sub(r'\\.{2,}|\\s{2,}', ' ', line)\n",
    "            line = re.sub(r'[^\\w\\s]|([ivxIVX]+\\b)', '', line)           \n",
    "            \n",
    "            # Tách câu nếu dòng không bắt đầu bằng số và kết hợp các dòng liên tiếp\n",
    "            if line.strip():  # Only process non-empty lines\n",
    "                if line[0].isupper() or line[0] in '“\"':\n",
    "                    if current_sentence:\n",
    "                        combined_lines.append((current_sentence.strip(), page_num))\n",
    "                    current_sentence = line.strip()\n",
    "                else:\n",
    "                    current_sentence += ' ' + line.strip()\n",
    "    \n",
    "    if current_sentence:\n",
    "        combined_lines.append((current_sentence.strip(), page_num))\n",
    "    \n",
    "    return combined_lines\n",
    "\n",
    "def save_combined_text_with_page_to_file(combined_text_with_page, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for i, (sentence, page_num) in enumerate(combined_text_with_page, start=1):\n",
    "            file.write(f\"Trang {page_num}, Câu {i}:{sentence}\\n\")\n",
    "\n",
    "# Kết hợp các dòng và tách câu, lưu cả số trang cho mỗi câu\n",
    "sentences_with_page = combine_lines_and_split_sentences(text_with_page)\n",
    "\n",
    "# Lưu nội dung vào file văn bản với số dòng và số trang\n",
    "save_combined_text_with_page_to_file(sentences_with_page, './output/sentence_split.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_word_sentences(sentences):\n",
    "    return [sentence for sentence in sentences if len(sentence[0].split()) > 1]\n",
    "\n",
    "def add_line_numbers(sentences_with_page):\n",
    "    numbered_sentences = []\n",
    "    for i, (sentence, page_num) in enumerate(sentences_with_page, start=1):\n",
    "        numbered_sentences.append(f\"Trang {page_num}: Câu {i}: {sentence}\")\n",
    "    return numbered_sentences\n",
    "\n",
    "def save_processed_sentences_to_file(sentences_with_page, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for sentence in sentences_with_page:\n",
    "            file.write(f\"{sentence}\\n\")\n",
    "            \n",
    "# Loại bỏ các câu có ít hơn 1 từ\n",
    "processed_sentences = remove_single_word_sentences(sentences_with_page)\n",
    "\n",
    "# Thêm số dòng và số trang vào mỗi câu\n",
    "numbered_sentences = add_line_numbers(processed_sentences)\n",
    "\n",
    "# Lưu nội dung vào file văn bản\n",
    "save_processed_sentences_to_file(numbered_sentences, './output/processed_sentences.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Sử dụng vectorizer để biến đổi văn bản mới\u001b[39;00m\n\u001b[0;32m     10\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml 0 0 2 1 5 4\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m new_features \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# In ra ma trận đặc trưng của văn bản mới\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_features\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m, in \u001b[0;36mtfidf_features\u001b[1;34m(texts)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtfidf_features\u001b[39m(texts):\n\u001b[0;32m      5\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 6\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Chuẩn bị dữ liệu văn bản\n",
    "def tfidf_features(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    features = vectorizer.fit_transform(texts)\n",
    "    return features\n",
    "\n",
    "# Sử dụng vectorizer để biến đổi văn bản mới\n",
    "new_texts = [\"l 0 0 2 1 5 4\"]\n",
    "new_features = tfidf_features(new_texts)\n",
    "\n",
    "# In ra ma trận đặc trưng của văn bản mới\n",
    "print(new_features.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danh sách từ dừng tự định nghĩa:\n",
      "{'các', 'để', 'cho', 'của', 'làm', 'và', 'là', 'với'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Vietnamese spaCy model\n",
    "nlp = spacy.blank(\"vi\")\n",
    "\n",
    "# Danh sách từ dừng tự định nghĩa\n",
    "custom_stop_words = {\"của\", \"và\", \"là\", \"các\", \"cho\", \"với\", \"làm\", \"để\"}  # Thêm các từ dừng khác cần thiết\n",
    "\n",
    "# In ra các từ dừng có sẵn trong vocab của spaCy\n",
    "if nlp.vocab and nlp.vocab.stop_words:\n",
    "    print(\"Danh sách từ dừng của mô hình spaCy:\")\n",
    "    print(nlp.vocab.stop_words)\n",
    "\n",
    "# Hoặc in ra các từ dừng từ danh sách tự định nghĩa\n",
    "print(\"Danh sách từ dừng tự định nghĩa:\")\n",
    "print(custom_stop_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
